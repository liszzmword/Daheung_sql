import "dotenv/config";
import fs from "fs";
import path from "path";
import { supabase } from "../lib/clients.mjs";
import { embedTexts } from "../lib/embedding.mjs";

/** í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í•  */
function chunkText(text, chunkSize = 1000, overlap = 150) {
  const chunks = [];
  let i = 0;
  while (i < text.length) {
    const end = Math.min(text.length, i + chunkSize);
    const chunk = text.slice(i, end).trim();
    if (chunk) chunks.push(chunk);
    i += chunkSize - overlap;
  }
  return chunks;
}

/** ë¬¸ì„œ í•˜ë‚˜ë¥¼ ì²­í‚¹ â†’ ì„ë² ë”© â†’ DB ì €ì¥ */
async function ingestDoc(docId, filePath) {
  const text = fs.readFileSync(filePath, "utf8");
  const chunks = chunkText(text);

  console.log(`  ${docId}: ${chunks.length}ê°œ ì²­í¬`);

  // ê¸°ì¡´ ë°ì´í„° ì‚­ì œ (ì¤‘ë³µ ë°©ì§€)
  const del = await supabase.from("rag_chunks").delete().eq("doc_id", docId);
  if (del.error) throw del.error;

  const embeddings = await embedTexts(chunks);

  for (let i = 0; i < chunks.length; i++) {
    const embeddingVector = embeddings[i].values || embeddings[i];
    const { error } = await supabase.from("rag_chunks").insert({
      doc_id: docId,
      chunk_idx: i,
      content: chunks[i],
      metadata: { source: filePath },
      embedding: embeddingVector,
    });
    if (error) throw error;
  }
}

async function main() {
  console.log("\nğŸ“¥ RAG ë¬¸ì„œ ì¸ì œìŠ¤íŠ¸ ì‹œì‘\n");

  const base = path.resolve("rag_docs");

  await ingestDoc("data_dictionary", path.join(base, "data_dictionary.md"));
  await ingestDoc("metrics", path.join(base, "metrics.md"));
  await ingestDoc("business_rules", path.join(base, "business_rules.md"));

  console.log("\nâœ… ì¸ì œìŠ¤íŠ¸ ì™„ë£Œ\n");
}

main().catch((e) => {
  console.error("âŒ ì˜¤ë¥˜:", e.message);
  process.exit(1);
});
